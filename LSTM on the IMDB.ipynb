{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Bidirectional LSTM on the IMDB sentiment classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python 2 Compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "from keras.models import Sequential\n",
    "# Obviousliy imports imbdb stuff\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are just layers to be stacked up on the network\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer | Docs | Summary | Questions\n",
    "---:|---\n",
    "Dense | <code>Dense</code> implements the operation: <code>output = activation(dot(input, kernel) + bias)</code> where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True). | It dots the matrix with a list of weights, and if it's greater than some threshold, it returns that matrix as 1, else returns 0 | What exactly is an activation function?\n",
    "Dropout | Dropout consists in randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting | I think this just adds some random 0's to the input, to add some stochasticity to the data. \"Dropout helps prevent weights from converging to identical positions. It does this by randomly turning nodes off when forward propagating. It then back-propagates with all the nodes turned on. Let’s take a closer look.\"| How exactly is it adding 0's, and why does that help avoid overfitting? \n",
    "Embedding | Turns positive integers (indexes) into dense vectors of fixed size. eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]] | I think this somehow re-representing the data we give to the sequnce as something interpretable by the other layers| What is a dense vector, and what is a vector's size (magnitude?)? \n",
    "LSTM | http://deeplearning.net/tutorial/lstm.html | So, normally when you let a layer have memory, the layer's weight will blow up or disappear whatever you give it. In LSTM, if the layer decided to loop back to itself, it doesn't change the input, avoiding that problem \"The key to LSTMs is the cell state, the horizontal line running through the top of the diagram. The cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. It’s very easy for information to just flow along it unchanged.\"| Lol, wtf. \n",
    "Bidirectional | BRNNs were introduced to increase the amount of input information available to the network. For example, multilayer perceptron (MLPs) and time delay neural network (TDNNs) have limitations on the input data flexibility, as they require their input data to be fixed. Standard recurrent neural network (RNNs) also have restrictions as the future input information cannot be reached from the current state. On the contrary, BRNNs do not require their input data to be fixed. | The recurrent layer can send information to previous iterations of the node, kind of like time travel | How is this different what just LSTM? Why is this apparently super useful? How does the layer know which prev iteration to send information?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The maximum number of charactaristics we consider for each input\n",
    "max_features = 20000\n",
    "\n",
    "# cut texts after this number of words\n",
    "# (among top max_features most common words)\n",
    "maxlen = 100\n",
    "\n",
    "# Iterate on your training data in batches. This trains the model 32 inputs at a time\n",
    "batch_size = 32\n",
    "\n",
    "# Not sure what x and y are, but puts x and y data into training and test data\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# Transform a list of num_samples sequences (lists of scalars) into a 2D Numpy array of shape (num_samples, num_timesteps).\n",
    "# Basically, turns these lists of numbers into matrices which the neural net would like to use\n",
    "# Obviously, x_train is used to train the model, x_test is used to test the model\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# Similar to what happens above.\n",
    "# Because I'm not savvy on the difference between x and y here, \n",
    "# not sure why not y_train = sequence.pad_sequences(y_train, maxlen=maxlen) \n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# This layer is here to convert the matrices above into input the neural net process\n",
    "# Obviously should come first\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "\n",
    "# This is the R in this RNN. The 64 is the dimensionality of the output space (which means what?)\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "\n",
    "# Adds some randomness in the training process.\n",
    "model.add(Dropout(0.5)\n",
    "          \n",
    "# This comes last because this is the threshold-cutter-offer\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Flattens the layers into something that can be trained\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "# Tunes the weights of the layers to produce the most accurate model possible\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=4,\n",
    "          validation_data=[x_test, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The model will add some training data, check the outputs, and adjust the weights of the LSTM layer, as well as randomly turn some inputs on and off in the dropout layer\n",
    "\n",
    "2. How do I make use or otherwise access the model?\n",
    "\n",
    "3. How does this program know to do sentiment analysis? Is that a property of something stored in the IMDB training data?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
